# Scale Sentry AI

AI-driven GitHub Action that reviews pull request diffs with OpenAI to forecast scalability risks before deploy.

## How It Works
- Collects the PR diff through the GitHub API.
- Runs lightweight heuristics to highlight potential hot spots (database calls, API handlers, loops, external requests, concurrency hints).
- Sends the truncated diff plus heuristic summary to OpenAI (default `gpt-4o`) with a structured prompt.
- Publishes a Markdown report summarising predicted bottlenecks, simulated load thresholds, optimisation ideas, and confidence.

## Action Inputs
| Name | Required | Default | Description |
| --- | --- | --- | --- |
| `github-token` | yes | ? | Token with `contents:read` and `issues:write` scopes (use `${{ secrets.GITHUB_TOKEN }}`). |
| `openai-api-key` | yes | ? | Secret OpenAI API key. |
| `openai-model` | no | `gpt-4o` | Chat completion model to call (e.g. `gpt-4o-mini`). |
| `openai-max-tokens` | no | `900` | Max tokens for the completion. Lower to control cost. |
| `openai-temperature` | no | `0.2` | Sampling temperature (0-1). |
| `target-language` | no | `TypeScript` | Hint about the repo's dominant language/framework. |
| `traffic-profile` | no | `1k-100k requests per second` | Target load envelope for simulation commentary. |
| `post-comment` | no | `true` | When `true`, post a PR comment with the report. |
| `write-job-summary` | no | `true` | When `true`, append the report to the workflow job summary. |

## Outputs
| Name | Description |
| --- | --- |
| `report` | Markdown body generated by the AI analysis. |

## Example Workflow
Save as `.github/workflows/scalability.yml`:

```yaml
name: Scalability Simulator

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  analyze:
    name: Analyze scalability
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Run Scale Sentry AI
        uses: ./
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          openai-api-key: ${{ secrets.OPENAI_API_KEY }}
          target-language: TypeScript
          traffic-profile: 1k-100k requests per second
```

## Development
1. Install dependencies: `npm install`.
2. Build the action: `npm run build` (outputs to `dist/`).
3. Run `npm run build -- --watch` while iterating to keep `dist` in sync.
4. Update `HEURISTIC_CHECKS` in `src/index.ts` to tune signal detection for your stack.

## Safety and Cost Notes
- Store `OPENAI_API_KEY` as a repository secret; the action never logs it.
- The diff passed to OpenAI is truncated to 12,000 characters to control token usage. The report will mention truncation when it occurs.
- Reduce `openai-max-tokens` or switch to a cheaper model (for example `gpt-4o-mini`) to limit spend.
- Set `post-comment` to `false` if you prefer to consume the report via outputs or job summary only.

## Extending
- Pipe the `report` output into follow-up jobs (for example, create artifacts or Slack notifications).
- Add lightweight load-test hooks (Locust, k6) and feed summary stats into the prompt.
- Swap or supplement the prompt with repository-specific context (architecture docs, service topology) for richer reasoning.
