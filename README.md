# Scale Sentry AI

*Predict scalability blowups before they happen.*

## Why You Will Love It
- Spots risky database access, heavy loops, and new endpoints the moment a PR opens.
- Summons OpenAI (`gpt-4o` by default) for simulated load analysis up to your chosen traffic profile.
- Prints developer-friendly Markdown that you can ship straight into pull request reviews.
- Works out of the box for TypeScript/Next.js stacks, yet stays flexible for any server runtime.

> Scale Sentry AI turns raw diffs into a battle plan for surviving launch day traffic.

## What Happens Under The Hood
1. Fetch the pull request diff with GitHub''s API.
2. Run heuristic detectors for hotspots (database, external calls, CPU, concurrency, loops, API routes).
3. Craft a focused prompt that includes the heuristics, traffic assumptions, and truncated diff.
4. Ask OpenAI for a structured Markdown report.
5. Post the result as a PR comment and job summary, and expose it via the `report` output.

## Quickstart Playbook

### Option A ? Inside This Repository
```yaml
name: Scalability Simulator

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  analyze:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: write
    steps:
      - uses: actions/checkout@v4
      - uses: ./
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          openai-api-key: ${{ secrets.OPENAI_API_KEY }}
          target-language: TypeScript
          traffic-profile: 1k-100k requests per second
```

### Option B ? Share With The World
1. Ensure `dist/` is committed, then tag a release: `git tag v1 && git push origin v1`.
2. Ask consumers to drop this step into their workflow:
   ```yaml
   - name: Run Scale Sentry AI
     uses: dextel2/scale-sentry-ai@v1
     with:
       github-token: ${{ secrets.GITHUB_TOKEN }}
       openai-api-key: ${{ secrets.OPENAI_API_KEY }}
       target-language: TypeScript
       traffic-profile: 1k-100k requests per second
   ```
3. Remind them to create an `OPENAI_API_KEY` secret and confirm the GitHub token can write PR comments.

## Configuration Cheat Sheet

| Input                | Required | Default                       | What It Controls                                                                           |
| -------------------- | -------- | ----------------------------- | ------------------------------------------------------------------------------------------ |
| `github-token`       | yes      | -                             | Auth for GitHub API calls and (optionally) commenting.                                     |
| `openai-api-key`     | yes      | -                             | Secret used to call the OpenAI Chat Completions API.                                       |
| `openai-model`       | no       | `gpt-4o`                      | Which OpenAI model to query. Try `gpt-4o-mini` for faster, cheaper runs.                   |
| `openai-max-tokens`  | no       | `900`                         | Completion budget. Drop this if you are cost-sensitive or using larger diffs.              |
| `openai-temperature` | no       | `0.2`                         | Creativity dial. Keep low for deterministic advice.                                        |
| `target-language`    | no       | `TypeScript`                  | Hint about your stack to steer the model.                                                  |
| `traffic-profile`    | no       | `1k-100k requests per second` | Range of load to simulate in the analysis narrative.                                       |
| `post-comment`       | no       | `true`                        | Set to `false` to skip PR comments and consume the report programmatically.                |
| `write-job-summary`  | no       | `true`                        | Tuck the report into the workflow run summary for easy viewing.                             |

### Output

| Name     | Description                                 |
| -------- | ------------------------------------------- |
| `report` | Full Markdown body generated by the analysis |

## Field Notes For Better Insights
- Large diffs are trimmed to 12,000 characters; the report calls out truncation so reviewers know the context was shortened.
- Heuristics are intentionally noisy. Tune `HEURISTIC_CHECKS` in `src/index.ts` to focus on the patterns that matter most in your stack (Prisma, Drizzle, Django ORM, custom SDKs, etc.).
- The structured Markdown response enables downstream automation (Slack alerts, Jira tickets, or dashboards). Pipe the `report` output into whatever you want next.

## Local Development Loop
1. Install dependencies: `npm install`.
2. Build the action: `npm run build` (writes to `dist/`).
3. Use `npm run build -- --watch` for fast iteration.
4. Add tests or validation scripts in `src/` if you extend the heuristics or prompt assembly.

## Safety And Cost Guardrails
- Never check secrets into git. Use GitHub repo or org secrets, and rotate often.
- Cap `openai-max-tokens` and prefer `gpt-4o-mini` when you just need quick signal.
- Disable commenting (`post-comment: "false"`) if your repository enforces manual reviews.
- Logs intentionally omit diff content and OpenAI payloads to keep reviews clean and compliant.

## Share Your Wins
- Ship a fork tailored for Python, Go, or JVM stacks.
- Feed real load-test telemetry back into the prompt for hybrid AI + empirical reports.
- Open an issue or PR with new heuristics so the community gets smarter with you.

Ready to ship? Commit `dist/`, tag a release, and let Scale Sentry AI guard your deploy runway.
